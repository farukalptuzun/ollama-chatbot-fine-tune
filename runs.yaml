E01:
  seq_len: 1024  # Optuna ile uyumlu (Optuna 1024 ile optimize etti, 4096 attention cost 4x artırır)
  lr_schedule: cosine
  learning_rate: 1.5e-5  # Optuna'dan %20 düşürülmüş (2M için güvenli)
  warmup_ratio: 0.03
  weight_decay: 0.05  # Optuna sonuçları: ~0.05 (0.1 LLaMA için agresif, uzun training'de underfitting yapar)
  rope_scaling: null
  batch_size: 6  # Optuna effective batch=12'yi koruyor (6*2=12)
  grad_acc: 2  # Optuna'dan transfer: batch=2, grad_acc=6 → batch=6, grad_acc=2
  num_epochs: 2  # Final training için 2 epoch

