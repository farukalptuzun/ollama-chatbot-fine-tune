# OPTUNA VERÄ°LERÄ°LERÄ°NDEN DÃ–NEN VERÄ°LER 
E01:
  seq_len: 1024
  lr_schedule: cosine
  learning_rate: 1.5e-5
  warmup_ratio: 0.035
  weight_decay: 0.05
  rope_scaling: null

  batch_size: 6
  grad_acc: 2
  num_epochs: 2

  eval_split_ratio: 0.01
  max_eval_samples: 2000
  eval_steps: 2000
  save_steps: 2000
  save_total_limit: 2
  logging_steps: 50
  seed: 42

# DAHA HIZLI EÄžÄ°TÄ°M Ä°Ã‡Ä°N
E01_fast:
  seq_len: 1024
  lr_schedule: cosine
  learning_rate: 1.5e-5
  warmup_ratio: 0.03
  weight_decay: 0.05
  rope_scaling: null

  batch_size: 6
  grad_acc: 2
  num_epochs: 2

  # ðŸ”¥ en bÃ¼yÃ¼k hÄ±z kazanÄ±mÄ± burada
  eval_split_ratio: 0.005        # %1 â†’ %0.5
  max_eval_samples: 1000         # 2000 â†’ 1000
  eval_steps: 6000               # 2000 â†’ 6000
  save_steps: 6000               # 2000 â†’ 6000
  save_total_limit: 2
  logging_steps: 100
  seed: 42

# 18-24 SAAT EÄžÄ°TÄ°M Ä°Ã‡Ä°N (270 saat â†’ ~20 saat)
E01_1day:
  seq_len: 768                   # 1024 â†’ 768 (%25-30 hÄ±z kazanÄ±mÄ±)
  lr_schedule: cosine
  learning_rate: 1.5e-5          # LR sabit tut (kalite korunur)
  warmup_ratio: 0.03
  weight_decay: 0.05
  rope_scaling: null

  batch_size: 8                  # 6 â†’ 8 (H100 80GB yeterli)
  grad_acc: 2
  num_epochs: 1                  # 2 â†’ 1 (Ã—0.5 sÃ¼re)
  max_steps: 40000               # Epoch yerine step-based (40K step â‰ˆ 17 saat)

  eval_split_ratio: 0.005        # KÃ¼Ã§Ã¼k eval seti
  max_eval_samples: 1000
  eval_steps: 8000               # Daha az evaluation
  save_steps: 8000
  save_total_limit: 2
  logging_steps: 100
  seed: 42
