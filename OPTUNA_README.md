# ğŸš€ Optuna ile Llama-3-8B Hiperparametre Optimizasyonu

Bu sistem, kurumsal chatbot iÃ§in Llama-3-8B modelini fine-tune ederken Optuna ile otomatik hiperparametre optimizasyonu yapar.

## ğŸ“‹ Ã–zellikler

- âœ… **Optuna ile Otomatik Optimizasyon**: TPE (Tree-structured Parzen Estimator) algoritmasÄ± ile en iyi hiperparametreleri bulur
- âœ… **KapsamlÄ± Metrikler**: 
  - Perplexity
  - Cross-Entropy Loss
  - F1 Score
  - Precision, Recall, Accuracy
- âœ… **A/B Test DesteÄŸi**: FarklÄ± konfigÃ¼rasyonlarÄ± karÅŸÄ±laÅŸtÄ±rma
- âœ… **JSON Ã‡Ä±ktÄ±**: TÃ¼m sonuÃ§lar JSON formatÄ±nda kaydedilir
- âœ… **Colab Uyumlu**: Google Colab'de Ã§alÄ±ÅŸmaya hazÄ±r

## ğŸ“¦ Kurulum

### Gereksinimler

```bash
pip install -r requirements.txt
```

Veya manuel olarak:

```bash
pip install torch transformers datasets accelerate optuna scikit-learn pyyaml sentencepiece numpy pandas
```

### HuggingFace Token

Llama-3-8B modelini kullanmak iÃ§in HuggingFace token'Ä±na ihtiyacÄ±nÄ±z var:

```bash
huggingface-cli login
```

## ğŸš€ KullanÄ±m

### 1. Dataset HazÄ±rlama

Dataset'iniz JSONL formatÄ±nda olmalÄ± ve her satÄ±rda bir JSON objesi bulunmalÄ±:

```json
{"text": "MÃ¼ÅŸteri: Merhaba, Ã¼rÃ¼nÃ¼nÃ¼z hakkÄ±nda bilgi alabilir miyim? Asistan: Tabii ki! Size nasÄ±l yardÄ±mcÄ± olabilirim?"}
{"text": "MÃ¼ÅŸteri: Fiyat bilgisi almak istiyorum. Asistan: Hangi Ã¼rÃ¼n iÃ§in fiyat bilgisi istiyorsunuz?"}
```

### 2. Komut SatÄ±rÄ±ndan Ã‡alÄ±ÅŸtÄ±rma

```bash
python optuna_optimization.py \
    --dataset data/train.jsonl \
    --n_trials 20 \
    --output optuna_results.json \
    --study_name llama3_8b_chatbot_optimization
```

### Parametreler

- `--dataset`: Dataset dosya yolu (varsayÄ±lan: `data/train.jsonl`)
- `--n_trials`: Deneme sayÄ±sÄ± (varsayÄ±lan: 10)
- `--output`: JSON Ã§Ä±ktÄ± dosyasÄ± (varsayÄ±lan: `optuna_results.json`)
- `--study_name`: Optuna study adÄ± (varsayÄ±lan: `llama3_8b_optimization`)
- `--timeout`: Timeout sÃ¼resi saniye cinsinden (opsiyonel)

### 3. Google Colab'de KullanÄ±m

1. `colab_optuna_training.ipynb` dosyasÄ±nÄ± Colab'e yÃ¼kleyin
2. Dataset'inizi yÃ¼kleyin veya Ã¶rnek dataset'i kullanÄ±n
3. `optuna_optimization.py` dosyasÄ±nÄ± Colab'e yÃ¼kleyin
4. Notebook'u hÃ¼cre hÃ¼cre Ã§alÄ±ÅŸtÄ±rÄ±n

## ğŸ“Š Optimize Edilen Hiperparametreler

Script aÅŸaÄŸÄ±daki hiperparametreleri optimize eder:

- **learning_rate**: 1e-5 ile 1e-3 arasÄ± (log scale)
- **batch_size**: 1, 2, 4
- **gradient_accumulation_steps**: 1 ile 8 arasÄ±
- **warmup_ratio**: 0.01 ile 0.1 arasÄ±
- **weight_decay**: 0.01 ile 0.3 arasÄ±
- **seq_len**: 2048, 4096, 8192
- **num_epochs**: 1 ile 3 arasÄ±

## ğŸ“ˆ Ã‡Ä±ktÄ± FormatÄ±

JSON Ã§Ä±ktÄ± dosyasÄ± ÅŸu yapÄ±da olacaktÄ±r:

```json
{
  "study_name": "llama3_8b_chatbot_optimization",
  "n_trials": 20,
  "best_trial": {
    "number": 5,
    "value": 12.34,
    "params": {
      "learning_rate": 2e-4,
      "batch_size": 2,
      "gradient_accumulation_steps": 4,
      "warmup_ratio": 0.05,
      "weight_decay": 0.1,
      "seq_len": 4096,
      "num_epochs": 2
    }
  },
  "best_metrics": {
    "perplexity": 15.23,
    "cross_entropy_loss": 2.72,
    "f1_score": 0.85,
    "precision": 0.83,
    "recall": 0.87,
    "accuracy": 0.85
  },
  "all_trials": [...]
}
```

## ğŸ” Metrikler AÃ§Ä±klamasÄ±

### Perplexity
Modelin tahmin belirsizliÄŸini Ã¶lÃ§er. DÃ¼ÅŸÃ¼k deÄŸer daha iyidir. `exp(eval_loss)` formÃ¼lÃ¼ ile hesaplanÄ±r.

### Cross-Entropy Loss
Modelin tahmin hatasÄ±nÄ± Ã¶lÃ§er. DÃ¼ÅŸÃ¼k deÄŸer daha iyidir. Validation loss olarak hesaplanÄ±r.

### F1 Score
Precision ve Recall'un harmonik ortalamasÄ±. YÃ¼ksek deÄŸer daha iyidir (0-1 arasÄ±).

### Accuracy
DoÄŸru tahmin yÃ¼zdesi. YÃ¼ksek deÄŸer daha iyidir (0-1 arasÄ±).

## ğŸ’¡ Ä°puÃ§larÄ±

1. **Trial SayÄ±sÄ±**: Daha iyi sonuÃ§lar iÃ§in en az 20-30 trial Ã¶nerilir
2. **GPU BelleÄŸi**: Llama-3-8B iÃ§in en az 16GB GPU belleÄŸi Ã¶nerilir
3. **Dataset Boyutu**: En az 1000 Ã¶rnek Ã¶nerilir, daha fazla daha iyi
4. **Zaman**: Her trial 30-60 dakika sÃ¼rebilir (GPU'ya baÄŸlÄ±)

## ğŸ› Sorun Giderme

### CUDA Out of Memory
- Batch size'Ä± azaltÄ±n (script otomatik olarak 1, 2, 4 dener)
- Gradient accumulation steps'i artÄ±rÄ±n
- Sequence length'i azaltÄ±n

### HuggingFace Token HatasÄ±
```bash
huggingface-cli login
```

### Dataset Format HatasÄ±
Dataset'iniz JSONL formatÄ±nda olmalÄ± ve her satÄ±rda `{"text": "..."}` formatÄ±nda olmalÄ±.

## ğŸ“ Ã–rnek KullanÄ±m

```python
import json

# SonuÃ§larÄ± yÃ¼kle
with open("optuna_results.json", "r") as f:
    results = json.load(f)

# En iyi parametreleri al
best_params = results["best_trial"]["params"]
print(f"En iyi learning rate: {best_params['learning_rate']}")
print(f"En iyi batch size: {best_params['batch_size']}")

# En iyi metrikleri gÃ¶rÃ¼ntÃ¼le
best_metrics = results["best_metrics"]
print(f"En iyi perplexity: {best_metrics['perplexity']:.2f}")
print(f"En iyi F1 score: {best_metrics['f1_score']:.4f}")
```

## ğŸ”„ Sonraki AdÄ±mlar

En iyi parametreleri bulduktan sonra:

1. `train.py` scriptinizi en iyi parametrelerle gÃ¼ncelleyin
2. Final modeli tÃ¼m dataset ile eÄŸitin
3. Modeli test edin ve deploy edin

## ğŸ“ Destek

Sorun yaÅŸarsanÄ±z:
- Script loglarÄ±nÄ± kontrol edin
- GPU bellek kullanÄ±mÄ±nÄ± izleyin
- Dataset formatÄ±nÄ± doÄŸrulayÄ±n

