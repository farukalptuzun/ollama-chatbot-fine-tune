# ğŸš€ Ollama Chatbot Fine-Tune

Kurumsal chatbot geliÅŸtirme iÃ§in **decoder-only** dil modellerini (Llama-3-8B, Qwen, vb.) fine-tune etmek Ã¼zere tasarlanmÄ±ÅŸ, esnek ve Ã¶lÃ§eklenebilir bir eÄŸitim framework'Ã¼.

## ğŸ“‹ Ä°Ã§indekiler

- [Ã–zellikler](#-Ã¶zellikler)
- [Proje YapÄ±sÄ±](#-proje-yapÄ±sÄ±)
- [Kurulum](#-kurulum)
- [HÄ±zlÄ± BaÅŸlangÄ±Ã§](#-hÄ±zlÄ±-baÅŸlangÄ±Ã§)
- [KullanÄ±m](#-kullanÄ±m)
- [Deney Matrisi Sistemi](#-deney-matrisi-sistemi)
- [Optuna ile Hiperparametre Optimizasyonu](#-optuna-ile-hiperparametre-optimizasyonu)
- [Dataset FormatÄ±](#-dataset-formatÄ±)
- [Checkpoint YÃ¶netimi](#-checkpoint-yÃ¶netimi)
- [Sorun Giderme](#-sorun-giderme)

## âœ¨ Ã–zellikler

### ğŸ¯ Ana Ã–zellikler

- âœ… **Deney Matrisi Sistemi**: YAML tabanlÄ± konfigÃ¼rasyon ile kolay deney yÃ¶netimi
- âœ… **Otomatik Checkpoint YÃ¶netimi**: Her deney iÃ§in ayrÄ± checkpoint dizini
- âœ… **HuggingFace Entegrasyonu**: TÃ¼m decoder-only modelleri destekler (Llama, Qwen, Mistral, vb.)
- âœ… **RoPE Scaling DesteÄŸi**: Uzun sequence length'ler iÃ§in RoPE scaling
- âœ… **Gradient Accumulation**: BÃ¼yÃ¼k batch size'larÄ± iÃ§in otomatik hesaplama
- âœ… **BF16 Training**: Bellek verimli mixed precision training
- âœ… **Error Handling**: KapsamlÄ± hata kontrolÃ¼ ve aÃ§Ä±klayÄ±cÄ± mesajlar

### ğŸ”¬ Optuna Optimizasyon

- âœ… **Otomatik Hiperparametre Optimizasyonu**: TPE (Tree-structured Parzen Estimator) algoritmasÄ±
- âœ… **KapsamlÄ± Metrikler**: Perplexity, Cross-Entropy Loss, F1 Score, Accuracy
- âœ… **A/B Test DesteÄŸi**: FarklÄ± konfigÃ¼rasyonlarÄ± karÅŸÄ±laÅŸtÄ±rma
- âœ… **JSON Ã‡Ä±ktÄ±**: TÃ¼m sonuÃ§lar JSON formatÄ±nda kaydedilir
- âœ… **Colab Uyumlu**: Google Colab'de Ã§alÄ±ÅŸmaya hazÄ±r

## ğŸ“ Proje YapÄ±sÄ±

```
ollama-chatbot-fine-tune/
â”œâ”€â”€ train.py                    # Ana eÄŸitim script'i
â”œâ”€â”€ runs.yaml                   # Deney matrisi konfigÃ¼rasyonu
â”œâ”€â”€ run_experiment.sh           # Deney Ã§alÄ±ÅŸtÄ±rma script'i
â”œâ”€â”€ optuna_optimization.py       # Optuna hiperparametre optimizasyonu
â”œâ”€â”€ setup.sh                    # Otomatik kurulum script'i
â”œâ”€â”€ requirements.txt            # Python baÄŸÄ±mlÄ±lÄ±klarÄ±
â”œâ”€â”€ colab_optuna_training.ipynb # Google Colab notebook
â”œâ”€â”€ OPTUNA_README.md            # Optuna kullanÄ±m kÄ±lavuzu
â”œâ”€â”€ data/
â”‚   â””â”€â”€ train.jsonl            # EÄŸitim dataset'i (JSONL format)
â””â”€â”€ models/
    â””â”€â”€ checkpoints/
        â””â”€â”€ E01/               # Deney checkpoint'leri
            â”œâ”€â”€ config.json
            â”œâ”€â”€ model.safetensors
            â””â”€â”€ tokenizer.json
```

## ğŸ› ï¸ Kurulum

### Gereksinimler

- Python 3.10+
- CUDA uyumlu GPU (Ã¶nerilir, en az 16GB VRAM)
- Git

### Otomatik Kurulum

```bash
# Projeyi klonlayÄ±n
git clone https://github.com/farukalptuzun/ollama-chatbot-fine-tune.git
cd ollama-chatbot-fine-tune

# Otomatik kurulum script'ini Ã§alÄ±ÅŸtÄ±rÄ±n
bash setup.sh
```

### Manuel Kurulum

```bash
# Virtual environment oluÅŸtur
python3 -m venv venv
source venv/bin/activate  # Linux/Mac
# veya
venv\Scripts\activate     # Windows

# BaÄŸÄ±mlÄ±lÄ±klarÄ± yÃ¼kle
pip install --upgrade pip
pip install -r requirements.txt

# Gerekli dizinleri oluÅŸtur
mkdir -p models/checkpoints data
```

### HuggingFace Token Ayarlama

Gated modeller (Llama-3-8B gibi) iÃ§in HuggingFace token gerekir:

```bash
huggingface-cli login
# veya
hf auth login
```

Token'Ä± oluÅŸturmak iÃ§in: https://huggingface.co/settings/tokens

## ğŸš€ HÄ±zlÄ± BaÅŸlangÄ±Ã§

### 1. Dataset HazÄ±rlama

Dataset'iniz JSONL formatÄ±nda olmalÄ±:

```bash
# data/train.jsonl Ã¶rneÄŸi
{"text": "MÃ¼ÅŸteri: Merhaba, Ã¼rÃ¼nÃ¼nÃ¼z hakkÄ±nda bilgi alabilir miyim?\nAsistan: Tabii ki! Size nasÄ±l yardÄ±mcÄ± olabilirim?"}
{"text": "MÃ¼ÅŸteri: Fiyat bilgisi almak istiyorum.\nAsistan: Hangi Ã¼rÃ¼n iÃ§in fiyat bilgisi istiyorsunuz?"}
```

### 2. Deney KonfigÃ¼rasyonu

`runs.yaml` dosyasÄ±na yeni deney ekleyin:

```yaml
E01:
  tokenizer: unigram
  vocab_size: 64000
  seq_len: 4096
  lr_schedule: cosine
  peak_lr: 3e-4
  warmup_ratio: 0.02
  weight_decay: 0.1
  rope_scaling: null
  tokens_per_step: 1048576
```

### 3. EÄŸitimi BaÅŸlatma

```bash
# Venv'i aktifleÅŸtir
source venv/bin/activate

# Deneyi Ã§alÄ±ÅŸtÄ±r
./run_experiment.sh E01
```

## ğŸ“– KullanÄ±m

### Temel KullanÄ±m

```bash
# Deney ID'si ile Ã§alÄ±ÅŸtÄ±r
./run_experiment.sh E01

# FarklÄ± model ile
python3 train.py \
  --run_id E01 \
  --model_name meta-llama/Meta-Llama-3-8B \
  --dataset data/train.jsonl
```

### Parametreler

#### `train.py` Parametreleri

- `--run_id` (required): `runs.yaml`'daki deney ID'si
- `--model_name` (default: `meta-llama/Meta-Llama-3-8B`): HuggingFace model adÄ± veya yerel path
- `--dataset` (default: `data/train.jsonl`): Dataset dosya yolu

#### `runs.yaml` KonfigÃ¼rasyon Parametreleri

- `tokenizer`: Tokenizer tipi (unigram, bpe) - ÅŸu an kullanÄ±lmÄ±yor, gelecekte kullanÄ±lacak
- `vocab_size`: Vocabulary boyutu - ÅŸu an kullanÄ±lmÄ±yor
- `seq_len`: Maksimum sequence length (2048, 4096, 8192, vb.)
- `lr_schedule`: Learning rate scheduler tipi (`cosine`, `linear`, `constant`)
- `peak_lr`: Peak learning rate (Ã¶rn: `3e-4`, `1e-4`)
- `warmup_ratio`: Warmup oranÄ± (0.0-1.0 arasÄ±, Ã¶rn: `0.02`)
- `weight_decay`: Weight decay deÄŸeri (Ã¶rn: `0.1`)
- `rope_scaling`: RoPE scaling faktÃ¶rÃ¼ (`null` veya sayÄ±sal deÄŸer)
- `tokens_per_step`: Her step'te iÅŸlenecek token sayÄ±sÄ± (batch size hesaplama iÃ§in)

## ğŸ§ª Deney Matrisi Sistemi

### Deney Ekleme

`runs.yaml` dosyasÄ±na yeni deney ekleyerek farklÄ± konfigÃ¼rasyonlarÄ± test edebilirsiniz:

```yaml
E01:
  tokenizer: unigram
  vocab_size: 64000
  seq_len: 4096
  lr_schedule: cosine
  peak_lr: 3e-4
  warmup_ratio: 0.02
  weight_decay: 0.1
  rope_scaling: null
  tokens_per_step: 1048576

E02:
  tokenizer: unigram
  vocab_size: 64000
  seq_len: 8192          # Daha uzun sequence
  lr_schedule: cosine
  peak_lr: 3e-4
  warmup_ratio: 0.02
  weight_decay: 0.1
  rope_scaling: null
  tokens_per_step: 1048576

E03:
  tokenizer: bpe
  vocab_size: 64000
  seq_len: 8192
  lr_schedule: cosine
  peak_lr: 1e-4         # Daha dÃ¼ÅŸÃ¼k learning rate
  warmup_ratio: 0.05    # Daha fazla warmup
  weight_decay: 0.1
  rope_scaling: 2.0     # RoPE scaling aktif
  tokens_per_step: 1048576
```

### Batch Size Hesaplama

Sistem otomatik olarak batch size'Ä± hesaplar:

```
gradient_accumulation_steps = tokens_per_step / (seq_len * world_size)
per_device_batch_size = 1 (sabit)
```

Ã–rnek:
- `tokens_per_step = 1048576`
- `seq_len = 4096`
- `world_size = 1` (tek GPU)
- `gradient_accumulation_steps = 1048576 / (4096 * 1) = 256`

## ğŸ”¬ Optuna ile Hiperparametre Optimizasyonu

### KullanÄ±m

```bash
python3 optuna_optimization.py \
    --dataset data/train.jsonl \
    --n_trials 20 \
    --output optuna_results.json \
    --study_name llama3_8b_chatbot_optimization
```

### Optimize Edilen Parametreler

- **learning_rate**: 1e-5 ile 1e-3 arasÄ± (log scale)
- **batch_size**: 1, 2, 4
- **gradient_accumulation_steps**: 1 ile 8 arasÄ±
- **warmup_ratio**: 0.01 ile 0.1 arasÄ±
- **weight_decay**: 0.01 ile 0.3 arasÄ±
- **seq_len**: 2048, 4096, 8192
- **num_epochs**: 1 ile 3 arasÄ±

### SonuÃ§larÄ± Ä°nceleme

```python
import json

# SonuÃ§larÄ± yÃ¼kle
with open("optuna_results.json", "r") as f:
    results = json.load(f)

# En iyi parametreleri al
best_params = results["best_trial"]["params"]
print(f"En iyi learning rate: {best_params['learning_rate']}")
print(f"En iyi batch size: {best_params['batch_size']}")

# En iyi metrikleri gÃ¶rÃ¼ntÃ¼le
best_metrics = results["best_metrics"]
print(f"En iyi perplexity: {best_metrics['perplexity']:.2f}")
print(f"En iyi F1 score: {best_metrics['f1_score']:.4f}")
```

DetaylÄ± bilgi iÃ§in: [OPTUNA_README.md](OPTUNA_README.md)

## ğŸ“Š Dataset FormatÄ±

### JSONL FormatÄ±

Her satÄ±r bir JSON objesi olmalÄ± ve `text` alanÄ± iÃ§ermeli:

```json
{"text": "TÃ¼rkiye bir Ã¼lkedir. BaÅŸkenti Ankara'dÄ±r."}
{"text": "Transformer models use self-attention mechanisms."}
{"text": "Python programlama dili veri bilimi alanÄ±nda yaygÄ±n kullanÄ±lÄ±r."}
```

### Chatbot FormatÄ± (Ã–nerilen)

```json
{"text": "MÃ¼ÅŸteri: Merhaba, Ã¼rÃ¼nÃ¼nÃ¼z hakkÄ±nda bilgi alabilir miyim?\nAsistan: Tabii ki! Size nasÄ±l yardÄ±mcÄ± olabilirim?\nMÃ¼ÅŸteri: Fiyat bilgisi almak istiyorum.\nAsistan: Hangi Ã¼rÃ¼n iÃ§in fiyat bilgisi istiyorsunuz?"}
```

### Dataset HazÄ±rlama Ä°puÃ§larÄ±

1. **Minimum Dataset Boyutu**: En az 1000 Ã¶rnek Ã¶nerilir
2. **Kalite > Miktar**: Az ama kaliteli veri, Ã§ok ama dÃ¼ÅŸÃ¼k kaliteli veriden daha iyidir
3. **Dengeli DaÄŸÄ±lÄ±m**: FarklÄ± konu ve senaryolarÄ± kapsamalÄ±
4. **Temizlik**: Gereksiz karakterler, HTML tag'leri temizlenmeli

## ğŸ’¾ Checkpoint YÃ¶netimi

### Checkpoint Konumu

Her deney iÃ§in checkpoint'ler ayrÄ± dizinde saklanÄ±r:

```
models/checkpoints/E01/
â”œâ”€â”€ config.json
â”œâ”€â”€ generation_config.json
â”œâ”€â”€ model.safetensors (veya model-*.safetensors)
â”œâ”€â”€ tokenizer.json
â”œâ”€â”€ tokenizer_config.json
â””â”€â”€ special_tokens_map.json
```

### Checkpoint Kullanma

```python
from transformers import AutoModelForCausalLM, AutoTokenizer

# Checkpoint'ten model yÃ¼kle
model = AutoModelForCausalLM.from_pretrained("models/checkpoints/E01")
tokenizer = AutoTokenizer.from_pretrained("models/checkpoints/E01")

# Inference
inputs = tokenizer("Merhaba, nasÄ±lsÄ±n?", return_tensors="pt")
outputs = model.generate(**inputs, max_length=100)
print(tokenizer.decode(outputs[0]))
```

### Checkpoint Yedekleme

```bash
# Checkpoint'i yedekle
tar -czf E01_checkpoint.tar.gz models/checkpoints/E01/

# Yedekten geri yÃ¼kle
tar -xzf E01_checkpoint.tar.gz
```

## ğŸ› Sorun Giderme

### HuggingFace Token HatasÄ±

```bash
# Token'Ä± kontrol et
hf auth whoami

# Yeni token ile giriÅŸ yap
huggingface-cli login
```

### CUDA Out of Memory

1. **Batch Size Azalt**: `tokens_per_step` deÄŸerini azaltÄ±n
2. **Sequence Length Azalt**: `seq_len` deÄŸerini dÃ¼ÅŸÃ¼rÃ¼n (4096 â†’ 2048)
3. **Gradient Checkpointing**: Model'e `gradient_checkpointing=True` ekleyin
4. **CPU Offloading**: `device_map="auto"` zaten aktif, daha fazla offload iÃ§in `accelerate` kullanÄ±n

### Dataset Format HatasÄ±

```bash
# Dataset'i kontrol et
head -n 5 data/train.jsonl

# Her satÄ±rÄ±n geÃ§erli JSON olduÄŸundan emin ol
python3 -c "import json; [print(json.loads(line)) for line in open('data/train.jsonl')]"
```

### Import HatasÄ± (sympy yavaÅŸ yÃ¼kleniyor)

Ä°lk import uzun sÃ¼rebilir (1-2 dakika). Bu normaldir. Sonraki Ã§alÄ±ÅŸtÄ±rmalarda hÄ±zlÄ± olacaktÄ±r.

```bash
# Cache'i Ã¶nceden oluÅŸtur
python3 -c "import sympy; print('sympy cache oluÅŸturuldu')"
```

### Python Komutu BulunamadÄ±

```bash
# Python3 kullan
python3 train.py --run_id E01

# Veya venv iÃ§indeki Python'u kullan
source venv/bin/activate
python train.py --run_id E01
```

## ğŸ“ˆ Performans Ä°puÃ§larÄ±

### GPU Bellek Optimizasyonu

1. **BF16 Training**: Zaten aktif (`bf16=True`)
2. **Gradient Accumulation**: BÃ¼yÃ¼k batch size'lar iÃ§in kullanÄ±lÄ±r
3. **CPU Offloading**: `device_map="auto"` ile otomatik
4. **Gradient Checkpointing**: Daha fazla bellek tasarrufu iÃ§in

### EÄŸitim HÄ±zlandÄ±rma

1. **Mixed Precision**: BF16 zaten aktif
2. **DataLoader Workers**: `num_workers` parametresi eklenebilir
3. **Compile Model**: PyTorch 2.0+ iÃ§in `model = torch.compile(model)`

## ğŸ”„ Sonraki AdÄ±mlar

1. **Optuna ile Optimizasyon**: En iyi hiperparametreleri bulun
2. **Final EÄŸitim**: En iyi parametrelerle tÃ¼m dataset ile eÄŸitin
3. **Model DeÄŸerlendirme**: Test seti ile modeli deÄŸerlendirin
4. **Deployment**: Modeli production'a alÄ±n (Ollama, vLLM, vb.)

## ğŸ“š Kaynaklar

- [HuggingFace Transformers](https://huggingface.co/docs/transformers)
- [Optuna Documentation](https://optuna.readthedocs.io/)
- [Llama 3 Model Card](https://huggingface.co/meta-llama/Meta-Llama-3-8B)
- [PyTorch Training Best Practices](https://pytorch.org/tutorials/beginner/introyt/trainingyt.html)

## ğŸ¤ KatkÄ±da Bulunma

1. Fork edin
2. Feature branch oluÅŸturun (`git checkout -b feature/amazing-feature`)
3. Commit edin (`git commit -m 'Add amazing feature'`)
4. Push edin (`git push origin feature/amazing-feature`)
5. Pull Request aÃ§Ä±n

## ğŸ“ Lisans

Bu proje MIT lisansÄ± altÄ±nda lisanslanmÄ±ÅŸtÄ±r.

## ğŸ‘¤ Yazar

**Faruk AlptÃ¼zÃ¼n**

- GitHub: [@farukalptuzun](https://github.com/farukalptuzun)

## ğŸ™ TeÅŸekkÃ¼rler

- HuggingFace ekibine Transformers kÃ¼tÃ¼phanesi iÃ§in
- Meta AI'ya Llama modelleri iÃ§in
- Optuna ekibine hiperparametre optimizasyonu iÃ§in

---

â­ Bu projeyi beÄŸendiyseniz yÄ±ldÄ±z vermeyi unutmayÄ±n!

