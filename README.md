# ğŸš€ Ollama Chatbot Fine-Tune

Kurumsal chatbot geliÅŸtirme iÃ§in **decoder-only** dil modellerini (Llama-3-8B, Qwen, vb.) fine-tune etmek Ã¼zere tasarlanmÄ±ÅŸ, esnek ve Ã¶lÃ§eklenebilir bir eÄŸitim framework'Ã¼.

## ğŸ“‹ Ä°Ã§indekiler

- [Ã–zellikler](#-Ã¶zellikler)
- [GeliÅŸtirme OrtamÄ±](#-geliÅŸtirme-ortamÄ±)
- [Proje YapÄ±sÄ±](#-proje-yapÄ±sÄ±)
- [Kurulum](#-kurulum)
- [HÄ±zlÄ± BaÅŸlangÄ±Ã§](#-hÄ±zlÄ±-baÅŸlangÄ±Ã§)
- [Workflow: Optuna + Final Training](#-workflow-optuna--final-training)
- [KullanÄ±m](#-kullanÄ±m)
- [Deney Matrisi Sistemi](#-deney-matrisi-sistemi)
- [Optuna ile Hiperparametre Optimizasyonu](#-optuna-ile-hiperparametre-optimizasyonu)
- [Dataset FormatÄ±](#-dataset-formatÄ±)
- [Checkpoint YÃ¶netimi](#-checkpoint-yÃ¶netimi)
- [Ollama Modelfile](#-ollama-modelfile)
- [Sorun Giderme](#-sorun-giderme)

## âœ¨ Ã–zellikler

### ğŸ¯ Ana Ã–zellikler

- âœ… **Optuna Trend Bulma**: KÃ¼Ã§Ã¼k dataset ile parametre aralÄ±ÄŸÄ± bulma (10K Ã¶rnek)
- âœ… **Final Training**: 2M Ã¶rnekle production-ready model eÄŸitimi
- âœ… **Deney Matrisi Sistemi**: YAML tabanlÄ± konfigÃ¼rasyon ile kolay deney yÃ¶netimi
- âœ… **Otomatik Checkpoint YÃ¶netimi**: Step-based + Epoch-based checkpoint sistemi
- âœ… **Resume Training**: EÄŸitim kopsa bile checkpoint'ten devam etme
- âœ… **HuggingFace Entegrasyonu**: TÃ¼m decoder-only modelleri destekler (Llama, Qwen, Mistral, vb.)
- âœ… **BF16 Training**: Bellek verimli mixed precision training
- âœ… **Gradient Checkpointing**: Bellek optimizasyonu
- âœ… **TensorCore Optimizasyonu**: `pad_to_multiple_of=8` ile hÄ±zlandÄ±rma
- âœ… **Group by Length**: Padding israfÄ±nÄ± azaltma
- âœ… **Evaluation DesteÄŸi**: Train/test split ve validation metrics

### ğŸ”¬ Optuna Optimizasyon

- âœ… **Trend Bulma Stratejisi**: En iyi parametre deÄŸil, transfer edilebilir parametre aralÄ±ÄŸÄ±
- âœ… **DaraltÄ±lmÄ±ÅŸ Search Space**: HÄ±zlÄ± ve verimli optimizasyon
- âœ… **Agresif Pruning**: MedianPruner ile erken durdurma
- âœ… **TPE Sampler**: Tree-structured Parzen Estimator algoritmasÄ±
- âœ… **KapsamlÄ± Metrikler**: Perplexity, Cross-Entropy Loss, F1 Score, Accuracy
- âœ… **JSON Ã‡Ä±ktÄ±**: TÃ¼m sonuÃ§lar JSON formatÄ±nda kaydedilir
- âœ… **Colab Uyumlu**: Google Colab'de Ã§alÄ±ÅŸmaya hazÄ±r

### âš¡ HÄ±z OptimizasyonlarÄ±

- âœ… **Max Steps DesteÄŸi**: Epoch yerine step-based training (18-24 saatlik eÄŸitim)
- âœ… **KÃ¼Ã§Ã¼k Sequence Length**: Optuna iÃ§in 1024, final iÃ§in 768 (hÄ±zlandÄ±rma)
- âœ… **Batch Size Optimizasyonu**: H100 80GB iÃ§in optimize edilmiÅŸ batch size
- âœ… **Evaluation Optimizasyonu**: Sabit kÃ¼Ã§Ã¼k eval seti (2M'de kritik)

## ğŸ–¥ï¸ GeliÅŸtirme OrtamÄ±

Bu proje **Google Colab** Ã¼zerinde geliÅŸtirilmiÅŸtir:

- **GPU**: H100 80GB VRAM
- **Sistem RAM**: 200GB
- **Python**: 3.10+
- **CUDA**: Uyumlu CUDA sÃ¼rÃ¼mÃ¼
- **Storage**: Google Drive entegrasyonu

### Colab Kurulumu

```python
# Google Drive'Ä± baÄŸla
from google.colab import drive
drive.mount('/content/drive')

# Projeye git
cd /content/drive/MyDrive/ollama-llm-fine-tune/ollama-chatbot-fine-tune

# BaÄŸÄ±mlÄ±lÄ±klarÄ± kur
pip install -r requirements.txt

# HuggingFace token
huggingface-cli login
```

## ğŸ“ Proje YapÄ±sÄ±

```
ollama-chatbot-fine-tune/
â”œâ”€â”€ train.py                    # Ana eÄŸitim script'i (2M final training)
â”œâ”€â”€ optuna_optimization.py       # Optuna hiperparametre optimizasyonu (10K trend bulma)
â”œâ”€â”€ runs.yaml                   # Deney matrisi konfigÃ¼rasyonu
â”œâ”€â”€ run_experiment.sh           # Deney Ã§alÄ±ÅŸtÄ±rma script'i
â”œâ”€â”€ setup.sh                    # Otomatik kurulum script'i
â”œâ”€â”€ requirements.txt            # Python baÄŸÄ±mlÄ±lÄ±klarÄ±
â”œâ”€â”€ colab_optuna_training.ipynb # Google Colab notebook
â”œâ”€â”€ OPTUNA_README.md            # Optuna kullanÄ±m kÄ±lavuzu
â”œâ”€â”€ Modelfile.E01_1day          # Ollama Modelfile (deployment iÃ§in)
â”œâ”€â”€ data/
â”‚   â””â”€â”€ train.jsonl            # EÄŸitim dataset'i (JSONL format, 2M Ã¶rnek)
â””â”€â”€ models/
    â””â”€â”€ checkpoints/
        â””â”€â”€ E01_1day/          # Deney checkpoint'leri
            â”œâ”€â”€ config.json
            â”œâ”€â”€ model.safetensors
            â”œâ”€â”€ tokenizer.json
            â”œâ”€â”€ checkpoint-8000/    # Step-based checkpoint'ler
            â”œâ”€â”€ checkpoint-16000/
            â”œâ”€â”€ epoch-1-final/      # Epoch-based checkpoint'ler
            â””â”€â”€ final-step-40000/   # Final checkpoint
```

## ğŸ› ï¸ Kurulum

### Gereksinimler

- Python 3.10+
- CUDA uyumlu GPU (Ã¶nerilir: H100 80GB veya en az 16GB VRAM)
- Sistem RAM: 200GB+ (bÃ¼yÃ¼k dataset'ler iÃ§in)
- Git

### Otomatik Kurulum

```bash
# Projeyi klonlayÄ±n
git clone https://github.com/farukalptuzun/ollama-chatbot-fine-tune.git
cd ollama-chatbot-fine-tune

# Otomatik kurulum script'ini Ã§alÄ±ÅŸtÄ±rÄ±n
bash setup.sh
```

### Manuel Kurulum

```bash
# Virtual environment oluÅŸtur
python3 -m venv venv
source venv/bin/activate  # Linux/Mac
# veya
venv\Scripts\activate     # Windows

# BaÄŸÄ±mlÄ±lÄ±klarÄ± yÃ¼kle
pip install --upgrade pip
pip install -r requirements.txt

# Gerekli dizinleri oluÅŸtur
mkdir -p models/checkpoints data
```

### HuggingFace Token Ayarlama

Gated modeller (Llama-3-8B gibi) iÃ§in HuggingFace token gerekir:

```bash
huggingface-cli login
# veya
hf auth login
```

Token'Ä± oluÅŸturmak iÃ§in: https://huggingface.co/settings/tokens

## ğŸš€ HÄ±zlÄ± BaÅŸlangÄ±Ã§

### 1. Dataset HazÄ±rlama

Dataset'iniz JSONL formatÄ±nda olmalÄ±:

```bash
# data/train.jsonl Ã¶rneÄŸi
{"text": "MÃ¼ÅŸteri: Merhaba, Ã¼rÃ¼nÃ¼nÃ¼z hakkÄ±nda bilgi alabilir miyim?\nAsistan: Tabii ki! Size nasÄ±l yardÄ±mcÄ± olabilirim?"}
{"text": "MÃ¼ÅŸteri: Fiyat bilgisi almak istiyorum.\nAsistan: Hangi Ã¼rÃ¼n iÃ§in fiyat bilgisi istiyorsunuz?"}
```

### 2. Deney KonfigÃ¼rasyonu

`runs.yaml` dosyasÄ±na yeni deney ekleyin. DetaylÄ± Ã¶rnekler iÃ§in [Deney Matrisi Sistemi](#-deney-matrisi-sistemi) bÃ¶lÃ¼mÃ¼ne bakÄ±n.

### 3. EÄŸitimi BaÅŸlatma

```bash
# Venv'i aktifleÅŸtir
source venv/bin/activate

# Deneyi Ã§alÄ±ÅŸtÄ±r
./run_experiment.sh E01_1day
```

## ğŸ”„ Workflow: Optuna + Final Training

Bu proje **iki aÅŸamalÄ± bir workflow** kullanÄ±r:

### AÅŸama 1: Optuna Optimizasyon (Trend Bulma)

**AmaÃ§**: 2M Ã¶rnekle Ã§alÄ±ÅŸacak parametre **aralÄ±ÄŸÄ±nÄ±** bulmak (mutlak en iyi deÄŸeri deÄŸil)

**Strateji**:
- KÃ¼Ã§Ã¼k dataset: 10K train, 2K validation
- KÄ±sa eÄŸitim: seq_len=1024, batch_size=2, 1 epoch
- Agresif pruning: MedianPruner(n_startup_trials=2, n_warmup_steps=20)
- DaraltÄ±lmÄ±ÅŸ search space: LR (1e-5, 5e-5), grad_acc (2-6), warmup (0.01-0.05)

**KullanÄ±m**:
```bash
python optuna_optimization.py \
  --n_trials 8 \
  --max_train_samples 10000 \
  --max_val_samples 2000
```

**SÃ¼re**: H100 ile 2-3 saat (8 trial)

### AÅŸama 2: Final Training (Production)

**AmaÃ§**: Optuna'dan gelen parametre aralÄ±ÄŸÄ±yla 2M Ã¶rnekle kaliteli model eÄŸitmek

**Strateji**:
- BÃ¼yÃ¼k dataset: 2M Ã¶rnek
- Optuna sonuÃ§larÄ±ndan transfer: LR %20 dÃ¼ÅŸÃ¼rÃ¼lmÃ¼ÅŸ (1.5e-5), effective batch korunuyor
- HÄ±z optimizasyonu: seq_len=768, max_steps=40000, batch_size=8
- Evaluation: Sabit kÃ¼Ã§Ã¼k eval seti (1000 Ã¶rnek)

**KullanÄ±m**:
```bash
python train.py --run_id E01_1day --dataset data/train.jsonl
```

**SÃ¼re**: H100 ile 18-24 saat (40K step)

**Kalite**: %85-90 korunur (production iÃ§in yeterli)

## ğŸ“– KullanÄ±m

### Temel KullanÄ±m

```bash
# Deney ID'si ile Ã§alÄ±ÅŸtÄ±r
./run_experiment.sh E01_1day

# Veya direkt Python ile
python train.py \
  --run_id E01_1day \
  --model_name meta-llama/Meta-Llama-3-8B \
  --dataset data/train.jsonl
```

### Parametreler

#### `train.py` Parametreleri

- `--run_id` (required): `runs.yaml`'daki deney ID'si
- `--model_name` (default: `meta-llama/Meta-Llama-3-8B`): HuggingFace model adÄ± veya yerel path
- `--dataset` (default: `data/train.jsonl`): Dataset dosya yolu

#### `runs.yaml` KonfigÃ¼rasyon Parametreleri

**Temel Parametreler**:
- `seq_len`: Maksimum sequence length (768, 1024, 2048, 4096, vb.)
- `lr_schedule`: Learning rate scheduler tipi (`cosine`, `linear`, `constant`)
- `learning_rate`: Learning rate (Ã¶rn: `1.5e-5`)
- `warmup_ratio`: Warmup oranÄ± (0.0-1.0 arasÄ±, Ã¶rn: `0.03`)
- `weight_decay`: Weight decay deÄŸeri (Ã¶rn: `0.05`)
- `rope_scaling`: RoPE scaling faktÃ¶rÃ¼ (`null` veya sayÄ±sal deÄŸer)

**Batch ve EÄŸitim**:
- `batch_size`: Per-device batch size (Ã¶rn: `8`)
- `grad_acc`: Gradient accumulation steps (Ã¶rn: `2`)
- `num_epochs`: Epoch sayÄ±sÄ± (Ã¶rn: `1`, `2`)
- `max_steps`: Maksimum step sayÄ±sÄ± (epoch override, opsiyonel)

**Evaluation ve Checkpoint**:
- `eval_split_ratio`: Validation split ratio (Ã¶rn: `0.01`)
- `max_eval_samples`: Maksimum eval Ã¶rnek sayÄ±sÄ± (Ã¶rn: `2000`)
- `eval_steps`: Her kaÃ§ step'te evaluation (Ã¶rn: `2000`)
- `save_steps`: Her kaÃ§ step'te checkpoint kaydet (Ã¶rn: `2000`)
- `save_total_limit`: Maksimum checkpoint sayÄ±sÄ± (Ã¶rn: `2`)
- `logging_steps`: Her kaÃ§ step'te log (Ã¶rn: `50`)
- `seed`: Random seed (Ã¶rn: `42`)

## ğŸ§ª Deney Matrisi Sistemi

### Ã–rnek KonfigÃ¼rasyonlar

#### E01: Optuna SonuÃ§larÄ±ndan Gelen Parametreler

```yaml
E01:
  seq_len: 1024                    # Optuna ile optimize edilmiÅŸ
  lr_schedule: cosine
  learning_rate: 1.5e-5           # Optuna'dan %20 dÃ¼ÅŸÃ¼rÃ¼lmÃ¼ÅŸ
  warmup_ratio: 0.035
  weight_decay: 0.05              # Optuna sonuÃ§larÄ±na uygun
  rope_scaling: null

  batch_size: 6                   # Effective batch=12 (6*2)
  grad_acc: 2
  num_epochs: 2

  eval_split_ratio: 0.01
  max_eval_samples: 2000
  eval_steps: 2000
  save_steps: 2000
  save_total_limit: 2
  logging_steps: 50
  seed: 42
```

#### E01_1day: 18-24 Saatlik HÄ±zlÄ± EÄŸitim

```yaml
E01_1day:
  seq_len: 768                    # 1024 â†’ 768 (%25-30 hÄ±z kazanÄ±mÄ±)
  lr_schedule: cosine
  learning_rate: 1.5e-5          # LR sabit tut (kalite korunur)
  warmup_ratio: 0.03
  weight_decay: 0.05
  rope_scaling: null

  batch_size: 8                  # 6 â†’ 8 (H100 80GB yeterli)
  grad_acc: 2
  num_epochs: 1                  # 2 â†’ 1 (Ã—0.5 sÃ¼re)
  max_steps: 40000               # Epoch yerine step-based (40K step â‰ˆ 17 saat)

  eval_split_ratio: 0.005        # KÃ¼Ã§Ã¼k eval seti
  max_eval_samples: 1000
  eval_steps: 8000               # Daha az evaluation
  save_steps: 8000
  save_total_limit: 2
  logging_steps: 100
  seed: 42
```

## ğŸ”¬ Optuna ile Hiperparametre Optimizasyonu

### Felsefe: Trend Bulma

**Ã–nemli**: Optuna'nÄ±n amacÄ± en iyi final loss deÄŸil, **2M eÄŸitimde iyi Ã§alÄ±ÅŸacak parametre aralÄ±ÄŸÄ±nÄ± bulmaktÄ±r**.

**AltÄ±n Kural**: 
- Optuna = trend bulur
- Final training = kaliteyi Ã¼retir

### KullanÄ±m

```bash
python optuna_optimization.py \
  --dataset data/train.jsonl \
  --n_trials 8 \
  --max_train_samples 10000 \
  --max_val_samples 2000 \
  --output optuna_results.json \
  --study_name llama3_8b_optimization
```

### Optimize Edilen Parametreler (DaraltÄ±lmÄ±ÅŸ Search Space)

- **learning_rate**: 1e-5 ile 5e-5 arasÄ± (log scale) - 2M iÃ§in ideal aralÄ±k
- **grad_acc**: 2 ile 6 arasÄ± - DaraltÄ±lmÄ±ÅŸ aralÄ±k
- **warmup_ratio**: 0.01 ile 0.05 arasÄ± - DaraltÄ±lmÄ±ÅŸ aralÄ±k
- **weight_decay**: 0.0 ile 0.1 arasÄ± - DaraltÄ±lmÄ±ÅŸ aralÄ±k
- **batch_size**: 2 (SABÄ°T) - Trend bulma iÃ§in yeterli
- **seq_len**: 1024 (SABÄ°T) - 2048 gereksiz pahalÄ±
- **num_epochs**: 1 (SABÄ°T) - Trend bulma iÃ§in yeterli

### Pruning KonfigÃ¼rasyonu

```python
MedianPruner(
    n_startup_trials=2,      # Ä°lk 2 trial'da pruning yok
    n_warmup_steps=20        # 20 step warmup
)

# TrainingArguments
eval_steps=200               # 10K Ã¶rnek iÃ§in ~6 pruning fÄ±rsatÄ±
logging_steps=50
```

### SonuÃ§larÄ± Ä°nceleme

```python
import json

# SonuÃ§larÄ± yÃ¼kle
with open("optuna_results.json", "r") as f:
    results = json.load(f)

# En iyi parametreleri al
best_params = results["best_trial"]["params"]
print(f"En iyi learning rate: {best_params['learning_rate']}")
print(f"En iyi grad_acc: {best_params['grad_acc']}")

# En iyi metrikleri gÃ¶rÃ¼ntÃ¼le
best_metrics = results["best_metrics"]
print(f"En iyi perplexity: {best_metrics['perplexity']:.2f}")
print(f"En iyi eval_loss: {best_metrics['eval_loss']:.4f}")
```

DetaylÄ± bilgi iÃ§in: [OPTUNA_README.md](OPTUNA_README.md)

## ğŸ“Š Dataset FormatÄ±

### JSONL FormatÄ±

Her satÄ±r bir JSON objesi olmalÄ± ve `text` alanÄ± iÃ§ermeli:

```json
{"text": "TÃ¼rkiye bir Ã¼lkedir. BaÅŸkenti Ankara'dÄ±r."}
{"text": "Transformer models use self-attention mechanisms."}
{"text": "Python programlama dili veri bilimi alanÄ±nda yaygÄ±n kullanÄ±lÄ±r."}
```

### Chatbot FormatÄ± (Ã–nerilen)

```json
{"text": "MÃ¼ÅŸteri: Merhaba, Ã¼rÃ¼nÃ¼nÃ¼z hakkÄ±nda bilgi alabilir miyim?\nAsistan: Tabii ki! Size nasÄ±l yardÄ±mcÄ± olabilirim?\nMÃ¼ÅŸteri: Fiyat bilgisi almak istiyorum.\nAsistan: Hangi Ã¼rÃ¼n iÃ§in fiyat bilgisi istiyorsunuz?"}
```

### Dataset HazÄ±rlama Ä°puÃ§larÄ±

1. **Minimum Dataset Boyutu**: Optuna iÃ§in 10K, final iÃ§in 2M+ Ã¶rnek Ã¶nerilir
2. **Kalite > Miktar**: Az ama kaliteli veri, Ã§ok ama dÃ¼ÅŸÃ¼k kaliteli veriden daha iyidir
3. **Dengeli DaÄŸÄ±lÄ±m**: FarklÄ± konu ve senaryolarÄ± kapsamalÄ±
4. **Temizlik**: Gereksiz karakterler, HTML tag'leri temizlenmeli

## ğŸ’¾ Checkpoint YÃ¶netimi

### Checkpoint TÃ¼rleri

Proje **Ã¼Ã§ tip checkpoint** kullanÄ±r:

1. **Step-based Checkpoint'ler**: Her `save_steps` step'te otomatik kayÄ±t
   - `checkpoint-8000/`, `checkpoint-16000/`, vb.
   - `save_total_limit` ile sÄ±nÄ±rlÄ± (son N checkpoint tutulur)

2. **Epoch-based Checkpoint'ler**: Her epoch bitiÅŸinde kayÄ±t
   - `epoch-1-final/`, `epoch-2-final/`
   - `CheckpointCallback` ile otomatik

3. **Final Checkpoint**: Training bitiÅŸinde (max_steps veya epoch tamamlanÄ±nca)
   - `final-step-40000/` veya `trainer.save_model()` ile kaydedilen model

### Checkpoint Konumu

Her deney iÃ§in checkpoint'ler ayrÄ± dizinde saklanÄ±r:

```
models/checkpoints/E01_1day/
â”œâ”€â”€ config.json
â”œâ”€â”€ generation_config.json
â”œâ”€â”€ model.safetensors (veya model-*.safetensors)
â”œâ”€â”€ tokenizer.json
â”œâ”€â”€ tokenizer_config.json
â”œâ”€â”€ checkpoint-8000/              # Step-based
â”œâ”€â”€ checkpoint-16000/
â”œâ”€â”€ epoch-1-final/                # Epoch-based
â””â”€â”€ final-step-40000/             # Final (max_steps)
```

### Resume Training

EÄŸitim kopsa bile checkpoint'ten devam eder:

```python
# Otomatik resume kontrolÃ¼
checkpoint = get_last_checkpoint(out_dir)
if checkpoint:
    print(f"ğŸ“‚ Checkpoint bulundu, devam ediliyor: {checkpoint}")
else:
    print("ğŸ†• Yeni eÄŸitim baÅŸlatÄ±lÄ±yor")

trainer.train(resume_from_checkpoint=checkpoint)
```

### Checkpoint Kullanma

```python
from transformers import AutoModelForCausalLM, AutoTokenizer

# Checkpoint'ten model yÃ¼kle
model = AutoModelForCausalLM.from_pretrained("models/checkpoints/E01_1day")
tokenizer = AutoTokenizer.from_pretrained("models/checkpoints/E01_1day")

# Inference
inputs = tokenizer("Merhaba, nasÄ±lsÄ±n?", return_tensors="pt").to(model.device)
outputs = model.generate(**inputs, max_length=100, temperature=0.7)
print(tokenizer.decode(outputs[0], skip_special_tokens=True))
```

### Checkpoint Yedekleme

```bash
# Checkpoint'i yedekle
tar -czf E01_1day_checkpoint.tar.gz models/checkpoints/E01_1day/

# Yedekten geri yÃ¼kle
tar -xzf E01_1day_checkpoint.tar.gz
```

## ğŸ¦™ Ollama Modelfile

Ollama ile deployment iÃ§in Modelfile kullanÄ±lÄ±r:

### Modelfile Ã–rneÄŸi

```dockerfile
FROM ./models/checkpoints/E01_1day/e01_1day.q8.gguf

PARAMETER temperature 0.7
PARAMETER top_p 0.9
PARAMETER repeat_penalty 1.1

SYSTEM """
Sen doÄŸal, akÄ±cÄ± ve mantÄ±klÄ± cevaplar veren bir sohbet asistanÄ±sÄ±n.
KÄ±sa ama anlamlÄ± cevaplar Ã¼ret.
Gereksiz tekrar yapma.
"""
```

### GGUF DÃ¶nÃ¼ÅŸÃ¼mÃ¼

**Ã–nemli**: Ollama GGUF formatÄ± gerektirir. HuggingFace formatÄ±nÄ± GGUF'ye dÃ¶nÃ¼ÅŸtÃ¼rmek iÃ§in:

```bash
# llama.cpp kullanarak
git clone https://github.com/ggerganov/llama.cpp.git
cd llama.cpp
make

# HuggingFace modelini GGUF'ye dÃ¶nÃ¼ÅŸtÃ¼r
python convert-hf-to-gguf.py \
  --outfile ./e01_1day.q8.gguf \
  --outtype q8_0 \
  models/checkpoints/E01_1day/
```

### Ollama'ya YÃ¼kleme

```bash
# Modelfile'Ä± Ollama'ya yÃ¼kle
ollama create e01_1day -f Modelfile.E01_1day

# Test et
ollama run e01_1day "Merhaba, nasÄ±lsÄ±n?"
```

## ğŸ› Sorun Giderme

### HuggingFace Token HatasÄ±

```bash
# Token'Ä± kontrol et
hf auth whoami

# Yeni token ile giriÅŸ yap
huggingface-cli login
```

### CUDA Out of Memory

**H100 80GB iÃ§in optimizasyonlar zaten aktif**:
1. **Gradient Checkpointing**: `gradient_checkpointing=True` aktif
2. **BF16 Training**: `bf16=True` aktif
3. **Device Map**: `device_map="auto"` ile otomatik offloading
4. **Batch Size**: H100 iÃ§in optimize edilmiÅŸ (batch_size=8, grad_acc=2)

**EÄŸer hala OOM alÄ±rsan**:
1. `batch_size`'Ä± azalt (8 â†’ 6 â†’ 4)
2. `seq_len`'i azalt (768 â†’ 512)
3. `grad_acc`'i artÄ±r (2 â†’ 4 â†’ 6)

### Dataset Format HatasÄ±

```bash
# Dataset'i kontrol et
head -n 5 data/train.jsonl

# Her satÄ±rÄ±n geÃ§erli JSON olduÄŸundan emin ol
python3 -c "import json; [print(json.loads(line)) for line in open('data/train.jsonl')]"
```

### Checkpoint Resume HatasÄ±

```bash
# Checkpoint dizinini kontrol et
ls -lh models/checkpoints/E01_1day/

# En son checkpoint'i manuel belirle
python train.py --run_id E01_1day \
  --resume_from_checkpoint models/checkpoints/E01_1day/checkpoint-32000
```

### Python Komutu BulunamadÄ±

```bash
# Python3 kullan
python3 train.py --run_id E01_1day

# Veya venv iÃ§indeki Python'u kullan
source venv/bin/activate
python train.py --run_id E01_1day
```

## ğŸ“ˆ Performans Ä°puÃ§larÄ±

### GPU Bellek Optimizasyonu (H100 80GB iÃ§in)

1. **BF16 Training**: Zaten aktif (`bf16=True`)
2. **Gradient Checkpointing**: Zaten aktif (`gradient_checkpointing=True`)
3. **Device Map**: Otomatik (`device_map="auto"`)
4. **Batch Size**: H100 iÃ§in optimize edilmiÅŸ (`batch_size=8`, `grad_acc=2`)

### EÄŸitim HÄ±zlandÄ±rma

1. **Sequence Length**: 768 kullan (1024 yerine %25-30 hÄ±z kazanÄ±mÄ±)
2. **Max Steps**: Epoch yerine step-based training (18-24 saat)
3. **TensorCore Optimizasyonu**: `pad_to_multiple_of=8` aktif
4. **Group by Length**: `group_by_length=True` ile padding israfÄ± azaltÄ±ldÄ±
5. **Evaluation Optimizasyonu**: Sabit kÃ¼Ã§Ã¼k eval seti (1000 Ã¶rnek)

### SÃ¼re OptimizasyonlarÄ± (270 saat â†’ 18-24 saat)

| Optimizasyon | Etki | SÃ¼re KazancÄ± |
|-------------|------|--------------|
| Epoch 2 â†’ 1 | Ã—0.5 | 135 saat |
| Max Steps 40K | Ã—0.3 | ~17 saat |
| Batch Size â†‘ | Ã—0.7 | - |
| Seq Len 768 | Ã—0.75 | - |
| **Toplam** | **~0.08** | **~21 saat** |

**Not**: Kalite %85-90 korunur (production iÃ§in yeterli)

## ğŸ”„ Sonraki AdÄ±mlar

1. **Optuna ile Optimizasyon**: 10K Ã¶rnekle parametre aralÄ±ÄŸÄ± bul (2-3 saat)
2. **Final Training**: 2M Ã¶rnekle production-ready model eÄŸit (18-24 saat)
3. **Model DeÄŸerlendirme**: Test seti ile modeli deÄŸerlendir
4. **GGUF DÃ¶nÃ¼ÅŸÃ¼mÃ¼**: HuggingFace â†’ GGUF formatÄ±na dÃ¶nÃ¼ÅŸtÃ¼r
5. **Ollama Deployment**: Modelfile ile Ollama'ya yÃ¼kle
6. **Production**: Modeli production'a al (API, chatbot, vb.)

## ğŸ“š Kaynaklar

- [HuggingFace Transformers](https://huggingface.co/docs/transformers)
- [Optuna Documentation](https://optuna.readthedocs.io/)
- [Llama 3 Model Card](https://huggingface.co/meta-llama/Meta-Llama-3-8B)
- [PyTorch Training Best Practices](https://pytorch.org/tutorials/beginner/introyt/trainingyt.html)
- [Ollama Documentation](https://github.com/ollama/ollama)
- [llama.cpp](https://github.com/ggerganov/llama.cpp) (GGUF dÃ¶nÃ¼ÅŸÃ¼mÃ¼ iÃ§in)

## ğŸ¤ KatkÄ±da Bulunma

1. Fork edin
2. Feature branch oluÅŸturun (`git checkout -b feature/amazing-feature`)
3. Commit edin (`git commit -m 'Add amazing feature'`)
4. Push edin (`git push origin feature/amazing-feature`)
5. Pull Request aÃ§Ä±n

## ğŸ“ Lisans

Bu proje MIT lisansÄ± altÄ±nda lisanslanmÄ±ÅŸtÄ±r.

## ğŸ‘¤ Yazar

**Faruk AlptÃ¼zÃ¼n**

- GitHub: [@farukalptuzun](https://github.com/farukalptuzun)

## ğŸ™ TeÅŸekkÃ¼rler

- HuggingFace ekibine Transformers kÃ¼tÃ¼phanesi iÃ§in
- Meta AI'ya Llama modelleri iÃ§in
- Optuna ekibine hiperparametre optimizasyonu iÃ§in
- Google Colab ekibine H100 GPU eriÅŸimi iÃ§in

---

â­ Bu projeyi beÄŸendiyseniz yÄ±ldÄ±z vermeyi unutmayÄ±n!
